

# XO GPT - User Query Paraphrasing Model


## Overview, Model Development Process and Performance



**FOR EVALUATION PURPOSES ONLY**

This document contains confidential and proprietary information of Kore.ai Inc. This document is provided solely for authorized recipients to evaluate the model. By accessing this document, you agree to use the information exclusively for evaluation, maintain strict confidentiality, and not disclose or share its contents with any third parties without our express written consent. You must implement appropriate security measures to prevent unauthorized access or dissemination of this information and promptly notify us of any unauthorized disclosure. This document does not grant any licenses, rights, or permissions regarding our intellectual property. 

**DISCLAIMER**

XO GPT is an advanced AI model designed to enhance your applications. However, like all emerging technologies, it will require improvements over time. Despite extensive testing, it is impossible to cover every possible scenario. As a result, XO GPT’s outputs may sometimes be unpredictable, leading to responses that could be inaccurate, biased, or otherwise unexpected. We strongly recommend that developers conduct thorough safety testing and make necessary adjustments to ensure the model fits their specific use cases.




# Live Versions


<table>
  <tr>
   <td>Model Version
   </td>
   <td>Base Model
   </td>
   <td>Languages Supported
   </td>
   <td>Deployed Region
   </td>
   <td>Deployment Date
   </td>
  </tr>
  <tr>
   <td rowspan="2" >



<a>Version 1.0</a>
   </td>
   <td rowspan="2" >Mistral 7B Instruct v0.2
   </td>
   <td rowspan="2" >English
   </td>
   <td>US
   </td>
   <td>1st Jun 2024
   </td>
  </tr>
  <tr>
   <td>DE
   </td>
   <td>3rd Sep 2024
   </td>
  </tr>
</table>





# Introduction

The XO-GPT User Query Paraphrasing Model is a cutting-edge AI solution that holds great potential for refining and enhancing chatbot interactions. The model ensures accurate and contextually appropriate responses by paraphrasing user queries to align with conversational context. This approach addresses key limitations of commercial LLMs, such as latency, cost-effectiveness, and data governance, making it an ideal choice for scalable and robust applications.

The model improves the naturalness and quality of chatbot responses by intelligently refining language and structure while preserving conversational context. This ensures that responses are more engaging, empathetic, and aligned with the user's intent. The XO-GPT User Query Paraphrasing Model fosters deeper user connections and enhances overall satisfaction by adapting to the user's emotions and conversational flow.

This technology is particularly beneficial for applications like customer support, virtual assistants, and interactive platforms, where effective communication directly impacts user engagement and loyalty. The XO-GPT User Query Paraphrasing Model sets a new standard for AI-driven conversational experiences by delivering accurate and beautifully crafted responses


## Audience

Large Language Models (LLMs) have wide-ranging impacts across an organization. Technical teams develop and integrate these models, while product and project managers leverage them for business goals. Researchers and documentation teams utilize LLM insights for advancement and clarity. Customer-facing and operational teams benefit from improved user interactions and system performance. Meanwhile, compliance, sales, and executive leadership consider LLMs' strategic value, ethical implications, and potential for driving innovation and competitive advantage.


# Problem Statement

In our ongoing efforts to enhance the capabilities of our conversational AI solutions, we recognized the limitations of relying solely on commercial large language models (LLMs) for user query paraphrasing. While effective, these models posed challenges related to latency, cost, data governance, and control over output quality. To overcome these hurdles, we initiated the fine-tuning of open-source models. This process involved generating synthetic data, validating it with AI, and incorporating human-curated datasets. Through this approach, we’ve significantly improved model performance while addressing the key issues of latency, cost, and data governance, which has provided us with greater control and customization over the models’ outputs.


## Challenges with Commercial Models



1. **Latency**: The time it takes for commercial LLMs to process and return a response can be significant, especially when dealing with high volumes of requests or real-time applications. This latency can impact user experience.
2. **Cost**: Commercial models often have a per-request cost, which can quickly become expensive, particularly as usage scales. This makes managing costs difficult, especially for large-scale deployments.
3. **Data Governance**: Sending user queries to external models raises data privacy and security concerns. This is especially important in industries that involve sensitive or proprietary information.
4. **Lack of Customization**: Commercial models are generally not tailored to specific use cases or industries, leading to less accurate or relevant responses.
5. **Limited Control**: There is minimal control over the internal workings of commercial models, making it difficult to correct or refine their behavior when they generate incorrect or undesirable outputs.
6. **Compliance and Regulatory Constraints**: Certain industries have stringent compliance and regulatory requirements that may not be fully supported by commercial LLM providers, complicating their use in those sectors.




# Model Overview


## Model Description

Our fine-tuned model is designed to meet the growing demands of sophisticated conversational applications. Building on robust open-source architectures, we have enhanced the model through a combination of synthetic data generation and rigorous human validation. This process has allowed us to improve the model’s accuracy, responsiveness, and overall performance. Moreover, by ensuring strict adherence to data governance protocols and optimizing for cost efficiency, our fine-tuned model offers a reliable and scalable solution that is fine-tuned to meet the unique needs of our customers. 


## Key Assumptions

Below are some key assumptions for the development of the XO-GPT User Query Paraphrasing Model. 



1. The model is designed to work only with text-based conversations.
2. The model paraphrases the user query only when it references or co-refers details from the previous conversation context. In all other cases, it does not paraphrase the user input.


## Benefits of XO-GPT User Query Paraphrasing Model

The XO-GPT Query Paraphrasing Model offers several potential advantages for businesses seeking to enhance their service capabilities:



* **Contextual Communication**: Adapts user query to the conversation context to accurately understand the user intent, fostering meaningful and satisfying interactions.
* **Cost-Effective Performance**: While striving for high performance, XO-GPT is designed to be a cost-effective solution. It currently supports English, and work is ongoing to expand its language capabilities.
* **Enhanced Data Security**: XO-GPT is designed with a strong focus on data protection, implementing robust security measures to help safeguard sensitive information.

!!! note

    The exact performance, features, and language support of the XO-GPT User Query Paraphrasing Model may vary based on specific implementations and use cases. We strongly recommend thorough testing in your particular environment to assess the model's suitability for your needs. This proactive approach will help ensure the model meets your expectations and requirements.







<img src="../images/xogptcsmodel.png" alt="Automations" title="image_Get a twilio phone number" style="border: 1px solid gray; zoom:70%;">



## Use Cases

The use cases of a user query paraphrasing span various domains, each benefiting from the model’s ability to provide accurate responses and seamless communication.


<table>
  <tr>
   <td><strong>Domain</strong>
   </td>
   <td><strong>Use Case</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="3" >Customer Support
   </td>
   <td>Simplify complex user queries in chatbots or virtual assistants for accurate intent detection.
   </td>
  </tr>
  <tr>
   <td>Remove ambiguous references to help match queries to the most relevant results.
   </td>
  </tr>
  <tr>
   <td>Enable contextual continuity in multi-turn conversations without losing clarity.
   </td>
  </tr>
  <tr>
   <td rowspan="3" >Healthcare
   </td>
   <td>Simplify complex patient inquiries for easier processing.
   </td>
  </tr>
  <tr>
   <td>Eliminate co-references in queries to ensure a precise understanding of patient history or ongoing treatments.
   </td>
  </tr>
  <tr>
   <td>Refine symptom-related questions for accurate analysis and recommendations by digital health tools.
   </td>
  </tr>
  <tr>
   <td rowspan="3" >Banking & Finance
   </td>
   <td>Clarify customer queries about account actions, ensuring seamless execution of financial tasks.
   </td>
  </tr>
  <tr>
   <td>Simplify follow-up queries regarding account details or previous interactions for better comprehension.
   </td>
  </tr>
  <tr>
   <td>Make ambiguous inquiries about financial products or services more straightforward for accurate responses.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Education
   </td>
   <td>Clarify multi-part or context-heavy student queries during online lessons for more practical guidance.
   </td>
  </tr>
  <tr>
   <td>Simplify questions about schedules, prerequisites, or course content for better clarity.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Human Resources
   </td>
   <td>Clarify ambiguous HR-related questions, such as benefits or leave policies, for accurate automated responses.
   </td>
  </tr>
  <tr>
   <td>Rephrase questions about workplace policies to ensure clarity in chatbot responses.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Legal
   </td>
   <td>Simplify user queries about legal contracts or policies to improve understanding.
   </td>
  </tr>
  <tr>
   <td>Make complex legal questions more straightforward for quick and accurate replies.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >E-commerce
   </td>
   <td>Rephrase follow-up queries about orders or shipments for accurate updates.
   </td>
  </tr>
  <tr>
   <td>Eliminate ambiguities in return or refund-related queries for smooth resolution.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Social Media
   </td>
   <td>Clarify questions about flagged content or platform policies to ensure proper resolution.
   </td>
  </tr>
  <tr>
   <td>Simplify user queries about account settings or privacy issues for accurate assistance.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >IT Support
   </td>
   <td>Rephrase vague or context-dependent user queries about technical issues for faster resolutions.
   </td>
  </tr>
  <tr>
   <td>Eliminate co-references in user-reported issues for clear problem identification.
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Travel & Hospitality
   </td>
   <td>Clarify multi-part or ambiguous booking inquiries to streamline the reservation process.
   </td>
  </tr>
  <tr>
   <td>Simplify user questions about changes or additions to travel plans for accurate responses.
   </td>
  </tr>
</table>



## Sample Outputs

The following section presents a few examples of the answers generated by the XO-GPT User Query Paraphrasing model based on the conversation history. 

**Input:**
```
User: Hi, can you help me select a university to study physics at?
Bot: Sure. Here are some of the top Universities for studying Physics: 1. Harvard University 2. MIT 3. Stanford 4. The University of Cambridge. Which sounds best to you?
User: Which one is best in fee structure?
Bot: Generally, the tuition fees for an Undergraduate course in Physics are most affordable at Stanford.
User: Ok, I'll choose that one.
```

**XO-GPT Response:**

User: Ok, I will choose to apply to Stanford University for a Physics course.


# XO-GPT - Model Building Process

The model-building process consists of several key stages that form the backbone of AI system development. It begins with data collection and processing, followed by model training or fine-tuning, where the AI learns from the prepared data. The final stages involve deploying the model into real-world environments and conducting ongoing evaluations to ensure performance and quality. These insights often feed back into earlier stages for continuous improvement.



* Data Collection: Gathering relevant datasets for training.
* Data Processing: Cleaning and preparing data for model use.
* Model Training/Fine-tuning: Optimizing model parameters using processed data.
* Model Deployment: Integrating the model into a production environment.
* Model Evaluation: Assessing model performance and standards compliance.


## Data Collection

We focus on data collection in the initial phase of developing our user query paraphrasing model. We gather a diverse and extensive set of samples from various sources, including knowledge bases, academic papers, and relevant articles. This collection is crucial as it forms the foundation of the information our model will use to generate accurate responses. Ensuring the quality and relevance of this data is key to the model's overall effectiveness.


### Training Data Source

The training data required to fine-tune and evaluate the model is created using the following stages:



* Synthetic data is created using the GPT 3.5 model from Azure Open AI.
* Human experts who understand the problem statement, challenges, and expected outcomes also manually develop data. 
* Another set of human annotators evaluates the data created from the above two steps. Evaluation includes checks for relevancy, coverage of various scenarios, correctness, etc. 
* No customer data is used to create either the training data or evaluate the model’s performance. 


### Training Data Profile

The training data consists of multiple samples across various categories and use cases. Subsequent sections of this document provide more details about the model's accuracy for different use categories. 

The complete training data is versioned and available in Kore.ai’s XO-GPT Data Repository. This data is proprietary to Kore.ai. Access to it is restricted to Kore.ai and can be made available to specific customers/partners/prospects only if needed.   


## 
Data Processing

Once the data is collected, we move on to data preprocessing. This involves cleaning the data to 



1. Remove any irrelevant or noisy content, 
2. Standardizing formats, 
3. Ensure the text is ready for use. 

We also perform tokenization and normalization to make the data compatible with the base model’s requirements. By carefully preparing the data, we aim to enhance the model’s ability to identify the aspect, the opinion, and the sentiment polarity.


## Model Training/Fine-tuning 

Training the User Query Paraphrasing model is an intricate process that we approach carefully, considering several factors. We use the prepared dataset to train the model, selecting training data that covers a broad spectrum of categories. Throughout the training process, we adjust hyperparameters such as learning rate, batch size, and the number of epochs to optimize the model’s performance. This iterative process involves frequent evaluations to ensure the model is learning effectively.

The XO-GPT model underwent a rigorous fine-tuning process to optimize its performance for User Query Paraphrasing tasks. Our approach leveraged several advanced techniques in machine learning and natural language processing:



* **Low-Rank Adaptation (LoRA)**: We applied LoRA to specific model layers, carefully tuning parameters such as rank, scaling factor, and dropout rate. This technique enables effective fine-tuning while minimizing the risk of overfitting.
* **Optimized Training Parameters:** The fine-tuning process utilized a set of carefully selected general parameters. These include an appropriate learning rate, batch size, and number of epochs, all chosen to balance efficient training with optimal performance. The model was configured to handle substantial input sequences, ensuring its capability to process complex queries and context.
* **Advanced Optimization Techniques**: We employed a state-of-the-art optimizer designed for large language models. Additional techniques such as warm-up steps, early stopping, and learning rate scheduling were considered to enhance the training process and model stability.
* **Task-Specific Adaptation:** The model was explicitly fine-tuned for causal language modeling tasks, focusing on question-answering capabilities. This targeted approach ensures that the XO-GPT model is well-suited for generating accurate and contextually relevant answers.


## Model Evaluation 

The following section describes the criteria used to evaluate the mode, the evaluation process, and the evaluation results. 


### Model Evaluation Criteria

We use a range of evaluation metrics to assess the model's performance, such as accuracy, fluency, hallucinations, robustness, AI Safety, Bias, etc. We also apply validation techniques, including cross-validation and hold-out validation, to ensure the model generalizes well to unseen data. Performance benchmarks help us measure how well the model performs compared to existing solutions or baseline models. This evaluation phase is critical for identifying areas where the model excels and where improvements are needed.


### Model Evaluation Process

To thoroughly evaluate the user query paraphrasing model, we employed a novel approach by generating synthetic data using a combination of GPT models and data created by human experts. We crafted a diverse set of data across various topics to simulate real-world scenarios. This data was designed to challenge the model with a wide range of samples, including those with intentional typo errors, poor grammar, and even profanity. The objective was to create a robust testing environment to assess how well the model handles imperfect inputs common in user interactions.

The evaluation process involved presenting these challenging utterances to our model and comparing its performance against several other models. We carefully monitored how each model responded to the synthetic data, focusing on its ability to interpret and generate the response despite the intentional imperfections correctly. By introducing a variety of Utterance types, we were able to test the model’s resilience, its ability to maintain accuracy under less-than-ideal conditions, and its capability to avoid generating inappropriate or harmful responses.


### Important Notes about Evaluation

While our evaluation shows strong performance in internal testing, we acknowledge that real-world performance may vary based on various other factors. 

The evaluation process also involved comparing XO-GPT's performance against several other models. While our internal testing indicates that XO-GPT performs favorably in certain areas, it's important to note that this comparison is based on a specific set of synthetic benchmarks and may not fully represent performance across all possible scenarios. The insights gained from this process were invaluable in refining the model to ensure it could deliver reliable, accurate, and safe responses. However, while the model has shown promising results in handling a variety of challenging inputs, further testing, and development are needed to fully assess its capabilities in handling very complex or domain-specific queries and potential edge cases that may not have been fully represented in our synthetic dataset.

It's important to note that various factors can influence latency measurements, including hardware specifications, network conditions, and the model's specific implementation. The provided latency figures should be interpreted within the context of our testing environment and may not directly apply to other deployments.

We are committed to enhancing XO-GPT's capabilities and will continue to focus on refining the model through ongoing testing and development, including incorporating real-world data and user feedback. 


# Model Benchmarks

This section highlights the features, updates, and changes that vary across different User Query Paraphrasing Model versions. It provides version-specific information that can help identify what is unique to each version. 

The following table summarizes the versions covered in this document:


| Model Version | Accuracy | Tokens/sec (TPS) | Latency (secs) | Benchmark Comparison | Test Data &Results  |
|---------------|----------|-----|----------------|---|---------|
| Version 1.0   | 97%      | 43 | 0.54  | [Benchmarks Summary](#benchmarks-summary)    | [Performance Benchmarking](#accuracy-benchmarking-xo-gpt-vs-other-models)   |




## Version 1.0


### Model Choice

We evaluate various community models that are suitable for the objective of response generation and fine-tune with our proprietary data described in the previous section. One or more candidate models are used throughout the training and evaluation phase. The model that performs better in terms of accuracy, safety, latency, etc., will be deployed. We continue to evaluate the models as part of ongoing improvements and may choose to use a different base model in the newer model versions. Currently, we are using [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) as one of the base models for fine-tuning and deployment.


<table>
  <tr>
   <td> Base Model
   </td>
   <td>Developer
   </td>
   <td>Language
   </td>
   <td>Release Date
   </td>
   <td>Status
   </td>
   <td>Knowledge Cutoff
   </td>
  </tr>
  <tr>
   <td>Mistral 7B Instruct v0.2
   </td>
   <td>Mistral AI
   </td>
   <td>Multi-lingual
   </td>
   <td>March, 2024
   </td>
   <td>Static 
   </td>
   <td>September, 2024
   </td>
  </tr>
</table>


Fine-tuning Parameters


<table>
  <tr>
   <td><strong>Parameters</strong>
   </td>
   <td><strong>Description</strong>
   </td>
   <td><strong>Value</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Load in 4-bit Precision</strong>
   </td>
   <td>Loads the model weights in 4-bit precision to reduce memory usage.	
   </td>
   <td>True
   </td>
  </tr>
  <tr>
   <td><strong>Use Double Quantization </strong>
   </td>
   <td>Uses double quantization to improve model accuracy.
   </td>
   <td>True
   </td>
  </tr>
  <tr>
   <td><strong>4-bit Quantization Type</strong>
   </td>
   <td>Type of quantization used for 4-bit precision.
   </td>
   <td>nf4
   </td>
  </tr>
  <tr>
   <td><strong>Computation Data Type</strong>
   </td>
   <td>The data type used for computation with 4-bit quantized weights.
   </td>
   <td>torch.float16
   </td>
  </tr>
  <tr>
   <td><strong>LoRA Rank</strong>
   </td>
   <td>The rank of the low-rank decomposition in LoRA.
   </td>
   <td>32
   </td>
  </tr>
  <tr>
   <td><strong>LoRA Alpha (Scaling Factor)</strong>
   </td>
   <td>LoRA scaling factor.
   </td>
   <td>16
   </td>
  </tr>
  <tr>
   <td><strong>LoRA Dropout Rate</strong>
   </td>
   <td>Dropout rate for LoRA layers to prevent overfitting.
   </td>
   <td>0.05
   </td>
  </tr>
  <tr>
   <td><strong>Bias Term Inclusion</strong>
   </td>
   <td>Specifies whether to add bias terms in the LoRA layers.
   </td>
   <td>-
   </td>
  </tr>
  <tr>
   <td><strong>Task Type</strong>
   </td>
   <td>The type of task for which LoRA is applied is Causal Language Modeling (CAUSAL_LM).
   </td>
   <td>CAUSAL_LM
   </td>
  </tr>
  <tr>
   <td><strong>Targeted Model Modules</strong>
   </td>
   <td>Specific layers in the model where LoRA is applied.
   </td>
   <td>["query_key_value"]
   </td>
  </tr>
</table>



#### General Parameters

The model is hosted on infrastructure with A10 - g5-xlarge. Some of the other general fine-tuning parameters include the following


<table>
  <tr>
   <td>Parameters
   </td>
   <td>Description
   </td>
   <td>Value
   </td>
  </tr>
  <tr>
   <td><strong>Learning Rate</strong>
   </td>
   <td>Controls how quickly or slowly the model reaches the minimum of loss.
   </td>
   <td>2e-4 (0.0002)
   </td>
  </tr>
  <tr>
   <td><strong>Batch Size</strong>
   </td>
   <td>A number of examples the model learns from at once.
   </td>
   <td>2
   </td>
  </tr>
  <tr>
   <td><strong>Epochs</strong>
   </td>
   <td>Number of times the model sees the entire training data.
   </td>
   <td>4 
   </td>
  </tr>
  <tr>
   <td><strong>Warm-up Steps</strong>
   </td>
   <td>Gradual start for the learning rate to help the model stabilize early on.
   </td>
   <td>–
   </td>
  </tr>
  <tr>
   <td><strong>Max Sequence Length</strong>
   </td>
   <td>The maximum length of input data the model can handle.
   </td>
   <td>32k 
   </td>
  </tr>
  <tr>
   <td><strong>Early Stopping</strong>
   </td>
   <td>Stops training if the model stops improving to prevent overfitting.
   </td>
   <td>–
   </td>
  </tr>
  <tr>
   <td><strong>Optimizer</strong>
   </td>
   <td>An algorithm to adjusts a model's parameters to improve performance.
   </td>
   <td>paged_adamw_8bit
   </td>
  </tr>
  <tr>
   <td><strong>Layer-wise LR Decay</strong>
   </td>
   <td>It uses different learning rates for other model parts to improve stability.
   </td>
   <td>–
   </td>
  </tr>
  <tr>
   <td><strong>Learning Rate Scheduler</strong>
   </td>
   <td>Adjust the learning rate during training to improve performance.
   </td>
   <td>–
   </td>
  </tr>
</table>



### Benchmarks Summary

To compare and contrast the performance of the fine-tuned model, we have considered the following other models: 



* Flan-T5: An open-source language model designed for fine-tuned performance across various natural language processing tasks, including summarization, translation, and conversational AI.
* GPT-4: OpenAI's advanced language model, known for exceptional reasoning and language generation across diverse tasks, including summarization, content creation, and conversational AI.

    <img src="../images/user-query-para-bencmark.png" alt="Automations" title="image_Get a twilio phone number" style="border: 1px solid gray; zoom:70%;">




In our internal evaluation with limited [sample data](#training-data-profile), XO-GPT achieved a 97% average score across key benchmarks, showcasing exceptional accuracy, fluency, and robustness performance. It effectively addressed linguistic challenges, including handling profanity and provocative content, highlighting its reliability and suitability for responsible AI applications.

Latency comparisons further demonstrated XO-GPT's superior efficiency. It achieved an average latency of just 0.54 seconds while maintaining high accuracy, outperforming models of similar size. With a token processing rate of 43 per second, XO-GPT delivers detailed responses without compromising speed or increasing latency.

**Key Takeaways**:



* Empathetic and Contextually Accurate: It addresses emotional and sensitive conversational contexts, such as Bias, Urgency, and Toxicity.
* Efficiency Leader: XO-GPT offers a quick response time (0.54 seconds) with compact output tokens, making it ideal for real-time applications.
* Best-in-Class Performance: With a 97% accuracy rate, XO-GPT surpasses Flan-T5 and GPT 4 models in most conversational categories, cementing its effectiveness and reliability.

By leveraging its strengths in performance, latency, and responsible AI principles, XO-GPT is well-positioned as a high-performing language model.


##### **Accuracy Benchmarking: XO GPT vs. Other Models**


<table>
  <tr>
   <td>Categories
   </td>
   <td>Total Samples
   </td>
   <td>XO GPT
   </td>
   <td>Flat-T5
   </td>
   <td>GPT 4
   </td>
  </tr>
  <tr>
   <td>switching the entity
   </td>
   <td>49
   </td>
   <td>90%
   </td>
   <td>80%
   </td>
   <td>100%
   </td>
  </tr>
  <tr>
   <td>switiching the intent
   </td>
   <td>39
   </td>
   <td>95%
   </td>
   <td>45%
   </td>
   <td>82%
   </td>
  </tr>
  <tr>
   <td>Selecting from a list - Mention the number index
   </td>
   <td>17
   </td>
   <td>100%
   </td>
   <td>88%
   </td>
   <td>100%
   </td>
  </tr>
  <tr>
   <td>Selecting from a list - Mention it by property of the item in the index
   </td>
   <td>24
   </td>
   <td>100%
   </td>
   <td>71%
   </td>
   <td>100%
   </td>
  </tr>
  <tr>
   <td>interruption
   </td>
   <td>20
   </td>
   <td>100%
   </td>
   <td>90%
   </td>
   <td>75%
   </td>
  </tr>
  <tr>
   <td>Normal flow
   </td>
   <td>19
   </td>
   <td>100%
   </td>
   <td>90%
   </td>
   <td>95%
   </td>
  </tr>
  <tr>
   <td>just following the flow
   </td>
   <td>20
   </td>
   <td>100%
   </td>
   <td>100%
   </td>
   <td>100%
   </td>
  </tr>
  <tr>
   <td>out of context
   </td>
   <td>20
   </td>
   <td>100%
   </td>
   <td>95%
   </td>
   <td>100%
   </td>
  </tr>
  <tr>
   <td><strong>Total</strong>
   </td>
   <td><strong>208</strong>
   </td>
   <td><strong>97%</strong>
   </td>
   <td><strong>78%</strong>
   </td>
   <td><strong>94%</strong>
   </td>
  </tr>
</table>



##### **Latency Benchmarking: XO GPT and Other Models**


<table>
  <tr>
   <td>Category
   </td>
   <td>XO GPT
   </td>
   <td>Flat-T5
   </td>
   <td>GPT-4
   </td>
  </tr>
  <tr>
   <td>Average
   </td>
   <td>0.54
   </td>
   <td>0.08
   </td>
   <td>0.81
   </td>
  </tr>
  <tr>
   <td>Median
   </td>
   <td>0.52
   </td>
   <td>0.07
   </td>
   <td>0.73
   </td>
  </tr>
  <tr>
   <td>Max
   </td>
   <td>1.02
   </td>
   <td>2.85
   </td>
   <td>4.13
   </td>
  </tr>
  <tr>
   <td>Min
   </td>
   <td>0.32
   </td>
   <td>0.03
   </td>
   <td>0.60
   </td>
  </tr>
  <tr>
   <td>TPS
   </td>
   <td>43
   </td>
   <td>268
   </td>
   <td>22
   </td>
  </tr>
</table>



# Model Roadmap


### Model Maintenance

Finally, ongoing model maintenance is essential for keeping the model up-to-date and effective. We regularly review and update the data, retrain the model as necessary, and address any issues or changes in the domain. This proactive approach helps ensure the model remains relevant and continues to provide accurate and valuable answers over time.


<table>
  <tr>
   <td><strong>Activities</strong>
   </td>
   <td><strong>Frequency</strong>
   </td>
  </tr>
  <tr>
   <td>New Features to the Models
   </td>
   <td>Quarterly
   </td>
  </tr>
  <tr>
   <td>Bug Fixes
   </td>
   <td>On Demand Basis
   </td>
  </tr>
  <tr>
   <td>Performance Improvement
   </td>
   <td>On Demand Basis
   </td>
  </tr>
</table>



### Model Expansion

The User Query Paraphrasing model is rapidly evolving to meet the growing demands of our customers. We are committed to enhancing its capabilities and expanding its reach. The following are some of the key upgrades planned as part of the roadmap: 



* Multilingual Proficiency: Beyond English, our model will soon be proficient in additional languages, enabling users from diverse backgrounds to benefit from its advanced capabilities.
* Aggregation: The ability to combine information from multiple sources to provide comprehensive and accurate answers.
* AI Safety: Annoying robust safeguards to ensure that our model generates ethical, unbiased responses and is free from harmful content.

The successful execution of the above roadmap depends on several factors. The rapidly changing technological landscape may necessitate adjustments to incorporate new advancements or overcome unforeseen challenges. Additionally, shifts in market demands or the competitive environment could lead to a reassessment of priorities. Finally, internal strategic decisions and unexpected technical hurdles could also impact the timeline and direction of the roadmap.

